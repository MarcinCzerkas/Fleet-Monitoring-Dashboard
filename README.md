# ‚öôÔ∏è Project Overview

This project describes a **business intelligence** initiative I delivered at my workplace in Q2 2025.

The objective was to design and implement an **end-to-end analytical solution** for a department that had been heavily reliant on Excel-based, manual reporting processes. I conducted a structured analysis of the underlying database, identified and modeled the relevant data domains, and developed a scalable reporting layer culminating in an interactive dashboard.

![Fleet_Monitoring_Dashboard](/Pictures/Dashboard%20demo.gif)

The solution replaced time-consuming, manual reporting cycles with an automated, near real-time reporting capability. Executives and analysts can now access up-to-date insights within minutes, significantly reducing reporting lead time from monthly or quarterly cycles to on-demand availability.

This transformation improved reporting efficiency, increased data reliability, and enabled faster, data-driven decision-making across the department.

# üí° Background

First of all, let me explain why there was a need for this project.

## üó®Ô∏è The story behind

I work at a car leasing company. The operating model is straightforward: vehicles are purchased from manufacturers, leased to clients, and typically sold once they return at the end of the contract. All operational events are recorded in the company‚Äôs transactional system, backed by a structured database.

Despite this, reporting had historically relied on predefined CSV exports generated by the system. These extracts were incomplete and fragmented, requiring analysts to manually consolidate multiple files in Excel. The process involved numerous workbooks, extensive cross-file dependencies, and heavy use of lookup formulas. Reporting cycles were time-consuming, error-prone, and difficult to scale or audit.

At that point, it became clear that the organization was underutilizing its core asset: the database itself. If the underlying data was already structured and centrally stored, there was no justification for maintaining a manual, spreadsheet-based reporting workflow. The opportunity was not to optimize Excel further, but to redesign the reporting approach entirely.

The initial phase was more complex than anticipated. The leasing lifecycle, including vehicle acquisition, contract management, returns, and resale, involves multiple status transitions and business rules. Understanding the full process required several iterations and close collaboration with business stakeholders.

After approximately two months of development‚Äîalongside my regular responsibilities‚ÄîI delivered a working analytical solution and presented it to my supervisors. What follows is a structured overview of selected elements of that solution.

The work can be logically divided into three major streams:

**1. Data wrangling**
Stakeholder alignment, process analysis, identification of relevant data domains within the database, and development of SQL queries to extract and structure the data model.

**2. Data transformation and preparation**
Data cleansing, harmonization, and shaping in Power Query to ensure consistency and analytical usability.

**3. Data modeling and visualization**
Development of the semantic model, DAX measures, and an executive dashboard designed for decision support.

Contrary to common perception, data cleansing was not the most time-intensive component. The primary complexity lay in accurately mapping business processes to the underlying data structures and designing a model that reflected operational reality.

‚ùï *One more disclaimer before we dive into the project: as it is a professional project and the data I worked on was highly sensitive, I can't share with you all details. I removed or anonymized all such sensitive information. As a result, the dashboard you see has some parts anonymized - just for demonstration purposes.*

## üõ†Ô∏è Tools I used

- **SQL** - helped me to extract from the system the data I wanted
- **Power BI** - where I visualized the data
- **Power Query** - I used it to clean and group the data in Power BI
- **DAX** - I used it for the calculations in Power BI
- **Oracle Analytics** - where I got the data from and wrote SQL queries
- **VS Code** - as writing SQL in Oracle Analytics is quite uncomfortable (lack of formatting and comments), I used VS Code to write and keep track of my SQL queries
- **Git** - I used it for the version control of the queries written in VS Code

# üîç Data Wrangling

### Business Process Analysis and Data Extraction

The first phase focused on developing a precise understanding of the business processes underpinning the data. I conducted structured discussions with multiple stakeholders across departments ‚Äî including Sales, Pricing, End of Contract, Controlling ‚Äî to capture different perspectives on how contracts progress through the lifecycle. Each statement was validated against information from other teams and reconciled with the data stored in the system.

Once I had established a coherent process view, I began developing SQL queries to extract relevant datasets. The initial approach was granular and validation-driven: I selected representative samples and reconciled records case by case, verifying that system data accurately reflected operational events. After confirming consistency at the transactional level, I aggregated results on a monthly basis and compared them against officially reported figures. With minor, explainable deviations, the alignment was strong ‚Äî confirming both the data logic and the business interpretation.

### Defining Reporting Logic for Key Milestones

The most complex issue at this stage was determining the correct reporting date for specific business milestones ‚Äî particularly when a contract should be classified as an *order* versus *production*.

The database contained numerous date fields representing different technical and operational events. However, none of them could be used in isolation to accurately represent reporting logic. The solution required constructing derived logic based on multiple conditions rather than relying on a single column.

To address this, I created several intermediate datasets using separate SELECT statements. Due to access limitations within Oracle Analytics ‚Äî where only Logical SQL was available ‚Äî I could not create database views or implement a fully integrated SQL-based transformation layer. Instead, I leveraged the Oracle Analytics UI to design a logical data model composed of five interconnected tables.

This approach enabled me to consolidate the relevant attributes into a single flattened dataset, which became the primary analytical source. The final dataset was exported and used as the structured input for the Power BI data model.

# üßπ Data Cleaning

The next phase involved importing the dataset into Power BI and transforming it using Power Query (M).

I implemented a **dimensional modeling** approach aligned with best practices. The model was designed as a classical star schema, with a single, well-defined fact table at its core and surrounding conformed dimensions providing analytical context. This ensured clear separation between transactional events and descriptive attributes, minimized redundancy, and optimized filter propagation.

Because the fact table represented an **accumulating snapshot process** with multiple business milestones, it contained several date columns corresponding to different lifecycle events. To handle this correctly, I implemented a **role-playing date dimension**. Multiple inactive relationships were created between the fact table and the date dimension, and the appropriate relationship was activated dynamically within DAX measures depending on the analytical context. This approach preserved semantic clarity while maintaining model performance and flexibility.

A significant portion of the work focused on ensuring data consistency and analytical reliability. I introduced additional derived columns to standardize categorical attributes where source data was inconsistent or fragmented across multiple fields. For example, certain classifications required conditional logic combining values from more than one column to determine a single, business-ready attribute.

I also implemented structured grouping logic, such as assigning vehicles to predefined age and mileage bands to enable controlled distribution analysis (e.g., histogram-style reporting) without relying on ad hoc calculations at the visualization layer.

To maintain readability and maintainability of the transformation layer, I structured the Power Query logic using nested let expressions and clearly separated transformation steps. This made the M code auditable, easier to extend, and aligned with the overall objective of building a robust, production-ready analytical model rather than a one-off report.

``` m
// GROUPING DATA INTO BINS

// Clean and group mileage
Mileage =
let
    TotalMileage = Table.AddColumn(ReplacedValues, "contractual_mileage",
        each Number.From([yearly_mileage]) / 12 * Number.From([contract_duration]),
        type number
    ),

    MileageBins = Table.AddColumn(
        TotalMileage, "mileage_bins",
        each if [contractual_mileage] <= 50000 then "<50k"
        else if [contractual_mileage] <= 100000 then "50k-100k"
        else if [contractual_mileage] <= 150000 then "100k-150k"
        else if [contractual_mileage] <= 200000 then "150k-200k"
        else if [contractual_mileage] > 200000 then ">200k"
        else "empty",
        type text
    ),

    MileageBinsSort = Table.AddColumn(
        MileageBins, "mileage_bins_sort",
        each if [mileage_bins] = "<50k" then 1
        else if [mileage_bins] = "50k-100k" then 2
        else if [mileage_bins] = "100k-150k" then 3
        else if [mileage_bins] = "150k-200k" then 4
        else if [mileage_bins] = ">200k" then 5
        else 0,
        Int64.Type
    ),

    YearlyMileageBins = Table.AddColumn(
        MileageBinsSort, "yearly_mileage_bins",
        each if Number.From([yearly_mileage]) <= 10000 then "<10k"
        else if Number.From([yearly_mileage]) <= 30000 then "10k-30k"
        else if Number.From([yearly_mileage]) <= 50000 then "30k-50k"
        else if Number.From([yearly_mileage]) <= 70000 then "50k-70k"
        else if Number.From([yearly_mileage]) > 70000 then ">70k"
        else "empty",
        type text
    ),

    YearlyMileageBinsSort = Table.AddColumn(
        YearlyMileageBins, "yearly_mileage_bins_sort",
        each if [yearly_mileage_bins] = "<10k" then 1
        else if [yearly_mileage_bins] = "10k-30k" then 2
        else if [yearly_mileage_bins] = "30k-50k" then 3
        else if [yearly_mileage_bins] = "50k-70k" then 4
        else if [yearly_mileage_bins] = ">70k" then 5
        else 0,
        Int64.Type
    )
in
    YearlyMileageBinsSort
```

# üìä Data Visualization

And here is the final product - my dashboard.

![DASHBOARD](/Pictures/Demo%201.png)
*The main page of the dashboard*

üí° I tried to focus on what is important and not to overload the user with too much information. However, I gave them the possibility to deep dive into the particular topics using **tooltips**, **swapping visuals** and the slicer panel:

![TOOLTIP](/Pictures/Demo%202.png)
*Tooltip showing the top 5 brands/models*

Should the user still feel overwhelmed and not know where to start from, they can use the **help button** to get familiarized with the dashboard page they're looking at:

![HELP](/Pictures/Demo%203.png)
*Help overlay displayed after clicking on the help button*

The DAX measures I wrote in this project involved using the following functions:

- CALCULATE (for context transition),
- USERELATIONSHIP (for activating the roles of the date dimension),
- DATESINPERIOD (for calculating results in a shifted time frame),
- SWITCH & SELECTEDVALUE (for dynamic swapping of the currently selected metrics),
- SUMX, AVERAGEX (for iterative row-level calculations),

and others.

# üí° Conclusion

My supervisors recognized this project as a game changer. ü•á

A manual, time-consuming workflow was replaced with an automated dashboard that delivers reliable, up-to-date insights with minimal effort.

The most important lesson was the value of deeply understanding the business before working with the data. Correct interpretation of processes and definitions proved more critical than the technical implementation itself.

A few months later, armed with this dashboard and the underlying semantic model I was able to deliver an analysis of highest importance for the Risk department helping identifying a potential significant loss. Thanks to my quick reaction an alert was raised early on before it was too late. ‚ö†Ô∏è

### What can be improved?

I kept working on this project and added new measures and KPIs. I created new report pages to deep dive into another details answering questions like: *How long does a contract last in reality in comparison to the contractual duration? Do we overestimate or underestimate the mileage on contract vs in reality?* I also integrated **Python** in the process to recalculate the residual value of the vehicles based on statistical data and measured on different age & mileage combinations.

## Appendix: who am I?

My name is **Marcin Czerkas**. I am a data professional specializing in Power BI, SQL and user-oriented analytical solutions.

I have been working in the data domain since 2023, when I transitioned into an analytical role and began supporting my team through process automation and dashboard development. As of June 2025, I am part of the Asset Risk department, where I focus on identifying risk patterns and emerging trends to support proactive decision-making and mitigate potential financial exposure. My role requires attending expert committees with top management as Risk representative in six Central European countries.

My transition into analytics marked the beginning of an ongoing learning process. Projects such as this one allow me to apply and refine my technical and analytical skills in real business environments while delivering measurable value.

### üîó **Visit [my LinkedIn profile](https://www.linkedin.com/in/marcin-czerkas-95150727a/) to learn more.**
